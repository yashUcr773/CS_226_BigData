# -*- coding: utf-8 -*-
"""final project 226.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V2mekjLlmHeCpAQ0HB0xEclI8to5EuTT
"""

# !pip install pyspark

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("AirBNB") \
    .getOrCreate()

from pyspark.sql import functions as F
from pathlib import Path

base_path = Path("/content/drive/MyDrive/CS 226 Final Project/usa")

from functools import reduce

# List all state folders
state_folders = [folder for folder in base_path.iterdir() if folder.is_dir()]

# Read all CSV files into a single DataFrame
dfs = []
for state_folder in state_folders:
    csv_path = state_folder / "listings_detailed.csv"
    if csv_path.is_file():
        df = spark.read.csv(str(csv_path), header=True, inferSchema=True)
        # Add a column for state (assuming the state name is the last part of the path)
        df = df.withColumn("state", F.lit(state_folder.name))
        dfs.append(df)

# Union all DataFrames into a single DataFrame
full_df = reduce(lambda df1, df2: df1.union(df2), dfs)

full_df.head()

df.show(5)

# Check schema
full_df.printSchema()

# Check number of rows
num_rows = full_df.count()
print(f"Number of rows: {num_rows}")

# Check number of columns
num_columns = len(full_df.columns)
print(f"Number of columns: {num_columns}")

# Define the path where you want to store the Parquet files
parquet_path = "/content/drive/MyDrive/CS 226 Final Project/parquet_data"

# Write DataFrame to Parquet
full_df.write.parquet(parquet_path)

# Print a message indicating where the Parquet files were saved
print(f"DataFrame has been written to Parquet format at: {parquet_path}")

# Create a temporary view for the DataFrame
full_df.createOrReplaceTempView('parquet_data')

# Execute SQL query with a WHERE clause using the indexed 'id' column
result = spark.sql("SELECT * FROM parquet_data WHERE id = 108061")

# Show the result
result.show()

# Repartition the DataFrame before writing to Parquet
num_partitions = 8  # Adjust the number based on your needs
full_df_new = full_df.repartition(num_partitions)

# Cache the DataFrame if it fits into memory
full_df_new.cache()

# Define the path where you want to store the Parquet files
parquet_path = "/content/drive/MyDrive/CS 226 Final Project/parquet_data"

# Write DataFrame to Parquet
full_df_new.write.parquet(parquet_path)

# Print a message indicating where the Parquet files were saved
print(f"DataFrame has been written to Parquet format at: {parquet_path}")

import os
import requests
import zipfile
from io import BytesIO
# parquet_path = "https://d1u36hdvoy9y69.cloudfront.net/cs-226-big-data/parquet_data.zip"

# URL to your hosted zip file
zip_file_url = "https://d1u36hdvoy9y69.cloudfront.net/cs-226-big-data/parquet_data.zip"

# Temporary directory to extract the zip file
temp_dir = "/tmp/parquet_data_temp"

# Ensure the temporary directory exists
os.makedirs(temp_dir, exist_ok=True)

# Download and extract the zip file
response = requests.get(zip_file_url)
with zipfile.ZipFile(BytesIO(response.content), 'r') as zip_ref:
    zip_ref.extractall(temp_dir)

# Path to the extracted Parquet data
parquet_path = os.path.join(temp_dir, "parquet_data")

parquet_data = spark.read.parquet(parquet_path)
# Create a temporary view for the DataFrame
parquet_data.createOrReplaceTempView('parquet_data2')

# Execute SQL query with a WHERE clause using the indexed 'id' column
result = spark.sql("SELECT * FROM parquet_data2 WHERE host_location = 'Phoenix, AZ' ")

# Show the result
result.show()

# Path to your Parquet data
parquet_path = "/content/drive/MyDrive/CS 226 Final Project/parquet_data3"

# Read Parquet files into a DataFrame
parquet_data = spark.read.parquet(parquet_path)

# Create a temporary view for the DataFrame
parquet_data.createOrReplaceTempView('parquet_data_view')

# Execute Spark SQL query
result = spark.sql("SELECT * FROM parquet_data2 WHERE host_name = 'Joyce'")

# Show the result
result.show()